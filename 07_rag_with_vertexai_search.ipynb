{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNe/hKjXtCFjEjShwmHP6MA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmadluay9/ADK-Training/blob/main/07_rag_with_vertexai_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-End RAG with Vertex AI Search and Gemini,\n",
        ",\n",
        "This tutorial demonstrates how to build a **Retrieval Augmented Generation (RAG)** pipeline using Google Cloud. We will cover three main steps:,\n",
        ",\n",
        "1.  **Ingestion**: Creating a Google Cloud Storage (GCS) bucket and uploading data.,\n",
        "2.  **Indexing**: Importing that data into a Vertex AI Search Data Store.,\n",
        "3.  **RAG**: Searching the data store for relevant context and using Gemini to answer questions.,\n",
        ",\n",
        "### Prerequisites,\n",
        "1.  A Google Cloud Project.,\n",
        "2.  The following APIs enabled:,\n",
        "    * Vertex AI API (`aiplatform.googleapis.com`),\n",
        "    * Discovery Engine API (`discoveryengine.googleapis.com`),\n",
        "    * Cloud Storage API (`storage.googleapis.com`),\n",
        "3.  **Important:** You must manually create a **Vertex AI Search App (Generic Type)** in the Google Cloud Console to get a `Data Store ID`. (Creation via API is possible but takes ~15-20 minutes, so we use an existing shell for this tutorial).\n",
        "\n",
        "<img height=\"650\" src=\"\n",
        "https://storage.cloud.google.com/training-public-eikon/search_agent_workflow.png\" alt=\"adk-search-diagram\" />"
      ],
      "metadata": {
        "id": "KAZjaJuaw53R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "YY6W658kxEsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Setup\n",
        "\n",
        "### 1.1 Install dependencies\n",
        "\n",
        "The Google Colab Notebooks environment includes a pre-installed version of the [`google-adk`](https://google.github.io/adk-docs/) library for Python and its required dependencies, so you don't need to install additional packages in this notebook.\n",
        "\n",
        "To install and use ADK in your own Python development environment outside of this course, you can do so by running:\n",
        "\n",
        "```\n",
        "pip install google-adk\n",
        "```"
      ],
      "metadata": {
        "id": "JMv1ixMCxNbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip show google-cloud-storage google-adk google-genai google-cloud-discoveryengine"
      ],
      "metadata": {
        "id": "egxRBsATfF-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install google-adk google-genai"
      ],
      "metadata": {
        "id": "gCjf78b2fpql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "\n",
        "# Set your project ID\n",
        "project_id = 'YOUR-PROJECT-ID' # @param {type:\"string\"}\n",
        "location = 'global' # @param {type:\"string\"}\n",
        "\n",
        "gcs_bucket_name = 'YOUR-BUCKET-NAME' # @param {type:\"string\"}\n",
        "gcs_location = 'asia-southeast2' # @param {type:\"string\"}\n",
        "\n",
        "data_store_id = 'YOUR-DATASTORE-ID' # @param {type:\"string\"}\n",
        "\n",
        "# Authenticate\n",
        "auth.authenticate_user(project_id=project_id)\n",
        "\n",
        "# Set the project for gcloud command-line tool\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "# Enable the required APIs\n",
        "!gcloud services enable aiplatform.googleapis.com discoveryengine.googleapis.com storage.googleapis.com"
      ],
      "metadata": {
        "id": "S-S7Cj9hSrzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_id_output = !gcloud config get project\n",
        "print(f\"PROJECT_ID = {project_id_output[0]}\")\n",
        "print(\"\")\n",
        "!gcloud auth list"
      ],
      "metadata": {
        "id": "KCXB__3eWMx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Te5Qcng_r4Oq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Data Ingestion (Google Cloud Storage)"
      ],
      "metadata": {
        "id": "r9Gsn09yrzzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create a GCS bucket and upload a sample policy document. This serves as the \"source of truth\" for our RAG pipeline."
      ],
      "metadata": {
        "id": "aQir3jF5r3hQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create sample text\n",
        "sample_text = \"\"\"\n",
        "City General Hospital: Patient Discharge Policy (2025)\n",
        "1. Discharge Planning: Planning begins upon admission. A case manager is assigned within 24 hours.\n",
        "2. Medication Reconciliation: A pharmacist must review all discharge medications with the patient before release.\n",
        "3. Follow-up Appointments: All appointments must be scheduled prior to the patient leaving the premises.\n",
        "4. Transportation: Patients undergoing sedation must have a verified escort for transport home; taxi/rideshare is not permitted alone.\n",
        "5. Documentation: The discharge summary must be completed in the EMR within 48 hours of release.\n",
        "    \"\"\"\n",
        "\n",
        "filename = \"discharge_policy.txt\"\n",
        "\n",
        "with open(filename, \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "print(f\"Saved local file: {filename}\")"
      ],
      "metadata": {
        "id": "IG_pw7Ci0yMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "storage_client = storage.Client(project=project_id)\n",
        "\n",
        "# Create Bucket\n",
        "print(f\"Creating bucket {gcs_bucket_name} in {gcs_location}...\")\n",
        "bucket = storage_client.create_bucket(gcs_bucket_name, location=gcs_location)\n",
        "print(f\"Created bucket: {bucket.name}\")"
      ],
      "metadata": {
        "id": "GqAJ6YQTry_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload to GCS\n",
        "\n",
        "# Define the destination path and filename inside the GCS bucket. \"blobs\" (Binary Large OBjects)\n",
        "blob_name = \"policies/discharge_policy_2025.txt\"\n",
        "\n",
        "# Create a blob object representing the file within the bucket\n",
        "blob = bucket.blob(blob_name)\n",
        "\n",
        "# Upload the contents of the local file to the GCS blob\n",
        "blob.upload_from_filename(filename)\n",
        "\n",
        "# Construct the standard GCS URI format (e.g., gs://my-bucket/policies/...)\n",
        "gcs_uri = f\"gs://{gcs_bucket_name}/{blob_name}\"\n",
        "\n",
        "# Print a confirmation message to the console\n",
        "print(f\"Successfully uploaded data to: {gcs_uri}\")"
      ],
      "metadata": {
        "id": "akMOs4gM1zFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "NGDHqsGi3WZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Create the Data Store\n",
        "\n",
        "Now we create a **Data Store** and trigger an ingestion job to index the files from GCS. This allows Vertex AI to perform semantic searches over your documents.\n",
        "\n",
        "[Documentation](https://docs.cloud.google.com/generative-ai-app-builder/docs/create-data-store-es#discoveryengine_v1_generated_DocumentService_ImportDocuments_sync-python)"
      ],
      "metadata": {
        "id": "f6K9eRRl5IHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core.client_options import ClientOptions\n",
        "from google.cloud import discoveryengine\n",
        "\n",
        "def create_data_store(project_id: str, location: str, data_store_id: str) -> str:\n",
        "    # Set endpoint if location is not global\n",
        "    client_options = (\n",
        "        ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
        "        if location != \"global\"\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    # Create a client\n",
        "    client = discoveryengine.DataStoreServiceClient(client_options=client_options)\n",
        "\n",
        "    # The full resource name of the collection\n",
        "    parent = client.collection_path(\n",
        "        project=project_id,\n",
        "        location=location,\n",
        "        collection=\"default_collection\",\n",
        "    )\n",
        "\n",
        "    data_store = discoveryengine.DataStore(\n",
        "        display_name=\"Demo Data Store\",\n",
        "\n",
        "        # Options: GENERIC, MEDIA, HEALTHCARE_FHIR\n",
        "        industry_vertical=discoveryengine.IndustryVertical.GENERIC,\n",
        "\n",
        "        # Options: SOLUTION_TYPE_RECOMMENDATION, SOLUTION_TYPE_SEARCH, SOLUTION_TYPE_CHAT, SOLUTION_TYPE_GENERATIVE_CHAT\n",
        "        solution_types=[discoveryengine.SolutionType.SOLUTION_TYPE_SEARCH],\n",
        "\n",
        "        # Options: NO_CONTENT, CONTENT_REQUIRED, PUBLIC_WEBSITE\n",
        "        content_config=discoveryengine.DataStore.ContentConfig.CONTENT_REQUIRED,\n",
        "    )\n",
        "\n",
        "    request = discoveryengine.CreateDataStoreRequest(\n",
        "        parent=parent,\n",
        "        data_store_id=data_store_id,\n",
        "        data_store=data_store,\n",
        "    )\n",
        "\n",
        "    # Make the request\n",
        "    print(f\"Creating Data Store '{data_store_id}'... This may take a moment.\")\n",
        "    operation = client.create_data_store(request=request)\n",
        "\n",
        "    print(f\"Waiting for operation to complete: {operation.operation.name}\")\n",
        "    response = operation.result()\n",
        "\n",
        "    metadata = discoveryengine.CreateDataStoreMetadata(operation.metadata)\n",
        "    print(\"\\nData Store Created Successfully!\")\n",
        "\n",
        "    return operation.operation.name\n",
        "\n",
        "# Execute the creation\n",
        "create_data_store(project_id, location, data_store_id)"
      ],
      "metadata": {
        "id": "gUnylv8V5NL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def import_documents_from_gcs(project_id: str, location: str, data_store_id: str, gcs_uri: str):\n",
        "    client_options = (\n",
        "        ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
        "        if location != \"global\"\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    # Create a client\n",
        "    client = discoveryengine.DocumentServiceClient(client_options=client_options)\n",
        "\n",
        "    # The full resource name of the search engine branch.\n",
        "    # e.g projects/{project}/locations/{location}/dataStores/{data_store_id}/branches/{branch}\n",
        "    parent = client.branch_path(\n",
        "        project=project_id,\n",
        "        location=location,\n",
        "        data_store=data_store_id,\n",
        "        branch=\"default_branch\",\n",
        "    )\n",
        "\n",
        "    request = discoveryengine.ImportDocumentsRequest(\n",
        "        parent=parent,\n",
        "        gcs_source=discoveryengine.GcsSource(\n",
        "            input_uris=[gcs_uri],\n",
        "\n",
        "            # **Options** :\n",
        "            # - `content` - Unstructured documents (PDF, HTML, DOC, TXT, PPTX)\n",
        "            # - `custom` - Unstructured documents with custom JSONL metadata\n",
        "            # - `document` - Structured documents in the discoveryengine.Document format.\n",
        "            # - `csv` - Unstructured documents with CSV metadata\n",
        "\n",
        "            data_schema=\"content\", # Using 'content' for unstructured documents (PDF, HTML, DOC, TXT, etc.)\n",
        "        ),\n",
        "        # Using INCREMENTAL to add new docs without deleting existing ones\n",
        "        reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL,\n",
        "    )\n",
        "\n",
        "    # Make the request\n",
        "    print(f\"Starting import job from {gcs_uri}...\")\n",
        "    operation = client.import_documents(request=request)\n",
        "\n",
        "    print(f\"Waiting for operation to complete: {operation.operation.name}\")\n",
        "    response = operation.result()\n",
        "\n",
        "    metadata = discoveryengine.ImportDocumentsMetadata(operation.metadata)\n",
        "\n",
        "    print(\"\\nImport Operation Complete!\")\n",
        "    print(f\"Success Count: {metadata.success_count}\")\n",
        "    print(f\"Failure Count: {metadata.failure_count}\")\n",
        "\n",
        "# Execute the import\n",
        "import_documents_from_gcs(project_id, location, data_store_id, gcs_uri)"
      ],
      "metadata": {
        "id": "o2ZA27LEStTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4 Agent Configuration (ADK)\n",
        "\n",
        "\n",
        "Using the **Agent Development Kit (ADK)**, we define an LLM agent that uses the VertexAiSearchTool to retrieve facts before answering.\n",
        "\n",
        "[Documentation](https://google.github.io/adk-docs/integrations/vertex-ai-search/)"
      ],
      "metadata": {
        "id": "eRYsoXDSSgo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Environment Configurations\n",
        "\n",
        "Here we bridge the Google Cloud settings into the Python environment variables. The ADK SDK uses these variables to authenticate and interact with Vertex AI models."
      ],
      "metadata": {
        "id": "t5wTokMIfUdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    # The SDK uses this ID for usage tracking and billing\n",
        "    os.environ['GOOGLE_CLOUD_PROJECT'] = project_id\n",
        "\n",
        "    # Defines the region where Vertex AI resources are hosted\n",
        "    os.environ['GOOGLE_CLOUD_LOCATION'] = location\n",
        "\n",
        "    # Directs the SDK to use Vertex AI infrastructure instead of the public Gemini API\n",
        "    os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = \"1\"\n",
        "\n",
        "    print(f\"✅ Environment configured for project: {project_id} in {location}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Configuration Error: {e}\")"
      ],
      "metadata": {
        "id": "6X_sY9SIXNpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Import Components & Initialize Asyncio\n",
        "Now, import the specific components you'll need from the Agent Development Kit and the Generative AI library. This keeps your code organized and ensures we have access to the necessary building blocks."
      ],
      "metadata": {
        "id": "xQqfFQQAfa_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai import types\n",
        "\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.models.google_llm import Gemini\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.tools import VertexAiSearchTool\n",
        "from google.adk.tools.agent_tool import AgentTool\n",
        "from google.adk.runners import InMemoryRunner\n",
        "\n",
        "from google.genai.types import GenerateContentConfig\n",
        "\n",
        "print(\"✅ ADK components imported successfully.\")"
      ],
      "metadata": {
        "id": "lD05yDQYe9wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3: Configure Retry Options\n",
        "When working with LLMs, you may encounter transient errors like rate limits or temporary service unavailability. Retry options automatically handle these failures by retrying the request with exponential backoff."
      ],
      "metadata": {
        "id": "8kPEmv9jg9ZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retry_config=types.HttpRetryOptions(\n",
        "    attempts=5,  # Maximum retry attempts\n",
        "    exp_base=7,  # Delay multiplier\n",
        "    initial_delay=1, # Initial delay before first retry (in seconds)\n",
        "    http_status_codes=[429, 500, 503, 504] # Retry on these HTTP errors\n",
        ")"
      ],
      "metadata": {
        "id": "S30-NjO4gxaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASTORE_PATH = f\"projects/{project_id}/locations/{location}/collections/default_collection/dataStores/{data_store_id}\"\n",
        "\n",
        "vertex_search_tool = VertexAiSearchTool(data_store_id=DATASTORE_PATH)"
      ],
      "metadata": {
        "id": "6KcTcVLsVudJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vertexai_search_agent = LlmAgent(\n",
        "    model=Gemini(\n",
        "        model=\"gemini-2.5-flash-lite\",\n",
        "        retry_options=retry_config\n",
        "    ),\n",
        "    name='vertexai_search_agent',\n",
        "    description='A helpful assistant for answering questions based on documents retrieved from Vertex AI Search.',\n",
        "    instruction=f\"\"\"\n",
        "    You are a helpful assistant that answers questions based on information found in the document store: {DATASTORE_PATH}.\n",
        "    Use the search tool to find relevant information before answering.\n",
        "    If the answer isn't in the documents, say that you couldn't find the information.\n",
        "    \"\"\",\n",
        "    tools=[vertex_search_tool],\n",
        "    generate_content_config=GenerateContentConfig(\n",
        "        temperature=0.1\n",
        "    )\n",
        ")\n",
        "\n",
        "search_tool = AgentTool(agent=vertexai_search_agent, skip_summarization=False)"
      ],
      "metadata": {
        "id": "zmQ0SPgkXa53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_agent = LlmAgent(\n",
        "    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n",
        "    name=\"root_agent\",\n",
        "    description=(\n",
        "        \"A supervisory agent that handles user queries and delegates document-based \"\n",
        "        \"questions to the Vertex AI Search agent when external knowledge retrieval is required.\"\n",
        "    ),\n",
        "    instruction=\"\"\"\n",
        "    You are the main orchestrator agent.\n",
        "\n",
        "    Your responsibilities:\n",
        "    1. Understand the user's question.\n",
        "    2. If the question requires information from the document store, call the `search_tool`.\n",
        "    3. Use the response from the `search_tool` to generate a clear, concise, and accurate final answer.\n",
        "    4. Do NOT fabricate information.\n",
        "    5. If the `search_tool` cannot find relevant information, clearly inform the user that the information is not available in the documents.\n",
        "    6. Keep responses professional and well-structured.\n",
        "\n",
        "    Only rely on verified information retrieved via the `search_tool` when answering document-based questions.\n",
        "    \"\"\",\n",
        "    tools=[search_tool],\n",
        ")"
      ],
      "metadata": {
        "id": "AxQBkI2uZc59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runner = InMemoryRunner(agent=root_agent)\n",
        "response = await runner.run_debug(\n",
        "    \"Can I take an Uber or taxi home after being sedated?\"\n",
        ")"
      ],
      "metadata": {
        "id": "lh5TrZboYI1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "KkW7B4EfaR8M"
      }
    }
  ]
}